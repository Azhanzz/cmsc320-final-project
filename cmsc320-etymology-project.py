# -*- coding: utf-8 -*-
"""Copy of CMSC320 Etymology Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZpX1F7ZlZIK1s7JuSJbijwjtaFVAt6H5

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from matplotlib import pyplot as plt

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# Initial Exploration
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from matplotlib import pyplot as plt


etymology = pd.read_csv('etymology.csv')
display(etymology)

etymology.columns

df = etymology[etymology['lang'] == 'English']
engl = df.drop(['lang', 'term_id', 'related_term_id', 'position', 'group_tag', 'parent_tag', 'parent_position'], axis = 1)
display(engl)

engl['related_lang'].value_counts().median()
engl['reltype'].unique()

# (engl['related_lang'] != 'English')
related_counts = engl[(engl['reltype'] == 'borrowed_from')]['related_lang'].value_counts()
piechart_lst = related_counts[related_counts >= 150]
other = pd.Series({'Other': engl[(engl['reltype'] == 'borrowed_from')]['related_lang'].value_counts()[engl['related_lang'].value_counts() < 150].sum()})
piechart_series = pd.concat([piechart_lst, other])
piechart_series

piechart_series.plot.pie(figsize = (10, 10), title = 'Number of English Words Borrowed from Each Language', fontsize = 8)

engl[engl['term'] == 'the']

freqs = pd.read_csv('unigram_freq.csv')
freqs.set_index('word', inplace = True)
freqs

# engl['frequency'] = engl['term'].apply(lambda word: freqs.loc[word] if (word in freqs.index) else -1)
df = engl.set_index('term').join(freqs).sort_values('count', ascending = False).dropna(subset = ['count'])
df['freq'] = df['count'] / freqs['count'].sum()
df

"""# Hypothesis 1

H0: There is no difference between the distribution of what languages the top 2000 loanwords have been borrowed from, the distribution of languages the top 2000-4000 loanwords have been borrowed from, and the distribution of languages loanwords other than the top 4000 have been borrowed from.

HA: There is a difference between the distribution of what languages the top 2000 loanwords have been borrowed from, the distribution of languages the top 2000-4000 loanwords have been borrowed from, and the distribution of languages loanwords other than the top 4000 have been borrowed from.
"""

df = pd.read_csv('etymology.csv')
borrowed_df = df[df['reltype'] == 'borrowed_from']
borrowed_df

top_2000_df = borrowed_df.iloc[:2000]
top_4000_df = borrowed_df.iloc[2000:4000]
other_df = borrowed_df.iloc[4000:]

top_2000_counts = top_2000_df['related_lang'].value_counts()
top_2000_lst = top_2000_counts[top_2000_counts >= 15]
top_2000_other = pd.Series({'Other': top_2000_counts[top_2000_counts < 15].sum()})
top_2000_series = pd.concat([top_2000_lst, top_2000_other])

top_4000_counts = top_4000_df['related_lang'].value_counts()
top_4000_lst = top_4000_counts[top_4000_counts >= 15]
top_4000_other = pd.Series({'Other': top_4000_counts[top_4000_counts < 15].sum()})
top_4000_series = pd.concat([top_4000_lst, top_4000_other])

other_counts = other_df['related_lang'].value_counts()
other_lst = other_counts[other_counts >= 20]
other_other = pd.Series({'Other': other_counts[other_counts < 20].sum()})
other_series = pd.concat([other_lst, other_other])

top_2000_series.plot.pie(figsize = (10, 10), title = 'Number of The Top 2000 English Loanwords Borrowed from Each Language', fontsize = 8)

top_4000_series.plot.pie(figsize = (10, 10), title = 'Number of The Top 2000-4000 English Loanwords Borrowed from Each Language', fontsize = 8)

other_series.plot.pie(figsize = (10, 10), title = 'Number of English Loanwords Other Than The Top 4000 Borrowed from Each Language', fontsize = 8)

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from matplotlib import pyplot as plt
from scipy.stats import f_oneway

etymology = pd.read_csv('etymology.csv')
df = etymology[etymology['lang'] == 'English']
engl = df.drop(['lang', 'term_id', 'related_term_id', 'position', 'group_tag', 'parent_tag', 'parent_position'], axis = 1)

freqs = pd.read_csv('unigram_freq.csv')
freqs.set_index('word', inplace = True)
df = engl.set_index('term').join(freqs).sort_values('count', ascending = False).dropna(subset = ['count'])
df['freq'] = df['count'] / freqs['count'].sum()
borrowed_df = df[df['reltype'] == 'borrowed_from']
borrowed_df
group1 = borrowed_df.iloc[:2000]
group2 = borrowed_df.iloc[2000:4000]
group3 = borrowed_df.iloc[4000:]

group1_counts = group1['related_lang'].value_counts()
group2_counts = group2['related_lang'].value_counts()
group3_counts = group3['related_lang'].value_counts()

all_languages = set(group1_counts.index) | set(group2_counts.index) | set(group3_counts.index)
group1_counts = group1_counts.reindex(all_languages, fill_value=0)
group2_counts = group2_counts.reindex(all_languages, fill_value=0)
group3_counts = group3_counts.reindex(all_languages, fill_value=0)

combined = pd.DataFrame({
    'Group 1': group1_counts,
    'Group 2': group2_counts,
    'Group 3': group3_counts
}).fillna(0)


combined['Group 3']*=(2000/4138)


combined = combined.sort_values(by='Group 1')
f_oneway(combined['Group 1'], combined['Group 2'], combined['Group 3'])

"""# Hypothesis 2

H0: There is no difference in the amount of language relations for more frequently used English words.

Ha: More frequently used English words have a significantly different (more or less) amount of language relations.
"""

#Hypothesis #2
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

ety_df = pd.read_csv('etymology.csv')

uni_df = pd.read_csv('unigram_freq.csv')

english_ety = ety_df[ety_df['lang'] == 'English'].groupby('term')['related_lang'].nunique()

new_df = pd.merge(uni_df, english_ety, left_on= 'word', right_on = 'term', how = 'inner')


import seaborn as sns






scatter_df = new_df[['count', 'related_lang']]

scatter_df = scatter_df.rename(columns={'count': 'x', 'related_lang': 'y'})

correlation = scatter_df['x'].corr(scatter_df['y'])

sns.lmplot(x='x', y='y', data=scatter_df, line_kws={'color': 'red'}, scatter_kws={'color': 'blue'})

plt.xlabel('Count in unigram_freq database')
plt.ylabel('# of related_languages')
plt.title('Correlation between frequency of words in English and the # of related languages to the word')

plt.show()

print(correlation)

from scipy.stats import ttest_ind

#split the data into two groups based on the median frequency
median_freq = scatter_df['x'].median()
high_freq = scatter_df[scatter_df['x'] > median_freq]['y']
low_freq = scatter_df[scatter_df['x'] <= median_freq]['y']

t_stat, p_value = ttest_ind(high_freq, low_freq)

print(f'T-statistic: {t_stat}\nP-value: {p_value}')

sns.kdeplot(high_freq, label='High Frequency', color='blue', fill=True, alpha=0.5)
sns.kdeplot(low_freq, label='Low Frequency', color='orange', fill=True, alpha=0.5)

#Lines for means
plt.axvline(high_freq.mean(), color='blue', linestyle='--', label='Mean High Freq')
plt.axvline(low_freq.mean(), color='orange', linestyle='--', label='Mean Low Freq')

# Step 3: Customize the plot
plt.title('Distribution of Related Languages for High and Low Frequency Words')
plt.xlabel('Number of Related Languages')
plt.ylabel('Density')
plt.legend()
plt.show()

#Remove outliers from scatter_df
Q1 = scatter_df['x'].quantile(0.25)
Q3 = scatter_df['x'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

cleaned_scatter_df = scatter_df[(scatter_df['x'] >= lower_bound) & (scatter_df['x'] <= upper_bound)]

high_freq = cleaned_scatter_df[cleaned_scatter_df['x'] > median_freq]['y']
low_freq = cleaned_scatter_df[cleaned_scatter_df['x'] <= median_freq]['y']

#Print new stats
correlation = cleaned_scatter_df['x'].corr(cleaned_scatter_df['y'])
print(f'Pearson correlation: {correlation}')

t_stat, p_value = ttest_ind(high_freq, low_freq)
print(f'T-statistic: {t_stat}\nP-value: {p_value}')

#Re-visualizing the distribution plot
sns.kdeplot(high_freq, label='High Frequency', color='blue', fill=True, alpha=0.5)
sns.kdeplot(low_freq, label='Low Frequency', color='orange', fill=True, alpha=0.5)
plt.axvline(high_freq.mean(), color='blue', linestyle='--', label='Mean High Frequency')
plt.axvline(low_freq.mean(), color='orange', linestyle='--', label='Mean Low Frequency')

plt.title('Distribution of Related Languages for High and Low Frequency Words (Cleaned Data)')
plt.xlabel('Number of Related Languages')
plt.ylabel('Density')
plt.legend()
plt.show()

"""**Conclusion for Hypothesis #2:**

Based on the statistical values calculated and the visuals from the graphs, it appears that there is a very small positive correlation between word frequency and amount of unique language relations a word has. The Pearson correlation coefficient is quite small, but upon decreasing extreme outliers does slightly increase, in line with this possibility.

Additionally, the P-values indicate there is no chance the differences found are coincidence. Given the high T-statistic this aligns with the idea that we should reject the null hypothesis, and determine that there is a relationship between word frequency and etymological language relations. That being said, the shapes of the distribution graphs, even after removing extreme outliers, have several modes and are high right-skewed, so this suggests further investigation is needed to determine the meaning of these statistics/conclusions.

It is likely that there is a clear positive relationship with certain commonly used parts of speech--words like "the" come to mind--which would undoubtedly have a rich etymological history.

Overall, the testing suggests a particularly significantly significant difference, the relationship is not strong, thus other factors likely are coming into play.

# Hypothesis 3#

H0: The distribution of relationship types in the dataset is uniform

HA: The distribution of relationship types in the dataset is not uniform.
Test: chi-square test for independence
"""

from scipy.stats import chi2_contingency

# load csv
df = pd.read_csv("etymology.csv")
reltype_counts = df['reltype'].value_counts()

# hypothesis test
observed = reltype_counts.values
n = sum(observed)
k = len(observed)
expected = np.array([n/k] * k)
chi2, p_value = chi2_contingency([observed, expected])[0:2]

print(f"Chi-square statistic: {chi2}")
print(f"p-value: {p_value}")

# bar
other = 0
labels = []
values = []

for reltype in reltype_counts.index:
  if reltype_counts[reltype] < 2000:
    other += reltype_counts[reltype]
  else:
    labels.append(reltype)
    values.append(reltype_counts[reltype])

plt.bar([*labels, "other"], [*values, other])
plt.xticks(rotation=80)
plt.title("Distribution of relationship types")
plt.xlabel("Relationship type")
plt.ylabel("Occurences")

"""**Conclusion for test 3**

We reject the null hypothesis that the distribution of relationship types is uniform. There is extremely strong statistical evidence to support the alternative hypothesis that the relationship types are not equally distributed in the dataset.
"""